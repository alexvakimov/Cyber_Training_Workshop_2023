#!/bin/sh
#SBATCH --partition=valhalla  --qos=valhalla
#SBATCH --clusters=faculty
##SBATCH --partition=general-compute  --qos=general-compute
##SBATCH --clusters=ub-hpc
##SBATCH --partition=scavenger  --qos=scavenger
#SBATCH --time=5-02:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=12
#SBATCH --cpus-per-task=1
#SBATCH --mem=60000
###SBATCH --mail-user=mshakiba@buffalo.edu
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST="$SLURM_JOB_NODELIST
echo "SLURM_NNODES="$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo "working directory="$SLURM_SUBMIT_DIR

# Here load whatever which is needed for running CP2K
module unload intel-mpi/2020.2

# For GNU compilers
source /projects/academic/cyberwksp21/Software/cp2k-intel/avx512-dependencies/cp2k-9.1-avx512/tools/toolchain/build/setup_gcc
source /projects/academic/cyberwksp21/Software/cp2k-intel/avx512-dependencies/cp2k-9.1-avx512/tools/toolchain/install/setup
module load mkl/2020.2

# For Intel compilers
#module load intel-mpi/2020.2
#module load intel/20.2

# Load VMD if cube visualization flag is turned on
module load cuda/5.5.22 
module load vmd

# For slurm environments
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
export OMP_NUM_THREADS=1

python run.py


