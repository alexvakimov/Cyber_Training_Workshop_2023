{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN) Basics: Error Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content <a name=\"TOC\"></a>\n",
    "\n",
    "1. [General setups](#setups)\n",
    "\n",
    "2. [Creation and initialization. Data members](#creation) \n",
    "\n",
    "3. [Saving and loading ANNs](#save_load)\n",
    "\n",
    "4. [Training](#training) \n",
    "\n",
    "5. [Error tracking](#errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Learning objectives\n",
    "\n",
    "- to create an ANN of arbitrary architecture\n",
    "- to initialize the ANN parameters\n",
    "- to construct an ANN training algorithm\n",
    "- to save and load the ANN state\n",
    "- to know the difference between the online and batch training of an ANN\n",
    "- to train ANN and track the progress\n",
    "- to track errors of the ANN training\n",
    "- to predict the outputs using the ANN and known inputs\n",
    "\n",
    "### B. Use cases\n",
    "\n",
    "- [Deep machine learning: Multilayer Perceptron](#mlp-1)\n",
    "- [Constructing ANN from file](#save_load-1)\n",
    "\n",
    "\n",
    "### C. Functions\n",
    "\n",
    "- `liblibra::liblinalg`\n",
    "  - [`pop_submatrix`](#pop_submatrix-1)\n",
    "\n",
    "- `liblibra::libspecialfunctions`\n",
    "  - [`randperm`](#randperm-1) | [also here](#randperm-2)\n",
    "\n",
    "  \n",
    "### D. Classes and class members\n",
    "\n",
    "- `liblibra::libann`\n",
    "  - [`NeuralNetwork`](#NeuralNetwork-1) | [also here](#NeuralNetwork-1)\n",
    "    - [`Nlayers`](#Nlayers-1)  \n",
    "    - [`Npe`](#Npe-1)\n",
    "    - [`W`](#W-1) | [also here](#W-2)\n",
    "    - [`dW`](#dW-1)\n",
    "    - [`dWold`](#dWold-1)    \n",
    "    - [`grad_w`](#grad_w-1) | [also here](#grad_w-2)\n",
    "    - [`grad_w_old`](#grad_w_old-1)\n",
    "    - [`B`](#B-1) | [also here](#B-2)\n",
    "    - [`dB`](#dB-1)\n",
    "    - [`dBold`](#dBold-1)\n",
    "    - [`grad_b`](#grad_b-1) | [also here](#grad_b-2)\n",
    "    - [`grad_b_old`](#grad_b_old-1)\n",
    "    - [`propagate`](#propagate-1) | [also here](#propagate-2)\n",
    "    - [`back_propagate`](#back_propagate-1) | [also here](#back_propagate-2)\n",
    "    - [`save`](#save-1)\n",
    "    - [`load`](#load-1)\n",
    "    - [`init_weights_biases_uniform`](#init_weights_biases_uniform-1)\n",
    "    - [`init_weights_biases_normal`](#init_weights_biases_normal-1)\n",
    "    - [`train`](#train-1)    \n",
    "    - [`error`](#error-1)\n",
    "\n",
    "- `liblibra::librandom`\n",
    "  - [`Random`](#Random-1)    \n",
    "    - [`normal`](#normal-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General setups\n",
    "<a name=\"setups\"></a> [Back to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/projects/academic/cyberwksp21/Software/Conda/Miniconda3/envs/libra/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import cmath\n",
    "import math\n",
    "import os\n",
    "\n",
    "if sys.platform==\"cygwin\":\n",
    "    from cyglibra_core import *\n",
    "elif sys.platform==\"linux\" or sys.platform==\"linux2\":\n",
    "    from liblibra_core import *\n",
    "import util.libutil as comn\n",
    "\n",
    "from libra_py import units\n",
    "from libra_py import data_outs\n",
    "import matplotlib.pyplot as plt   # plots\n",
    "#matplotlib.use('Agg')\n",
    "#%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "#from matplotlib.mlab import griddata\n",
    "\n",
    "plt.rc('axes', titlesize=24)      # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=20)      # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=20)     # legend fontsize\n",
    "plt.rc('xtick', labelsize=16)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=16)    # fontsize of the tick labels\n",
    "\n",
    "plt.rc('figure.subplot', left=0.2)\n",
    "plt.rc('figure.subplot', right=0.95)\n",
    "plt.rc('figure.subplot', bottom=0.13)\n",
    "plt.rc('figure.subplot', top=0.88)\n",
    "\n",
    "colors = {}\n",
    "\n",
    "colors.update({\"11\": \"#8b1a0e\"})  # red       \n",
    "colors.update({\"12\": \"#FF4500\"})  # orangered \n",
    "colors.update({\"13\": \"#B22222\"})  # firebrick \n",
    "colors.update({\"14\": \"#DC143C\"})  # crimson   \n",
    "\n",
    "colors.update({\"21\": \"#5e9c36\"})  # green\n",
    "colors.update({\"22\": \"#006400\"})  # darkgreen  \n",
    "colors.update({\"23\": \"#228B22\"})  # forestgreen\n",
    "colors.update({\"24\": \"#808000\"})  # olive      \n",
    "\n",
    "colors.update({\"31\": \"#8A2BE2\"})  # blueviolet\n",
    "colors.update({\"32\": \"#00008B\"})  # darkblue  \n",
    "\n",
    "colors.update({\"41\": \"#2F4F4F\"})  # darkslategray\n",
    "\n",
    "clrs_index = [\"11\", \"21\", \"31\", \"41\", \"12\", \"22\", \"32\", \"13\",\"23\", \"14\", \"24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation and initialization. Data members.\n",
    "<a name=\"creation\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "Create the ANN object with 3 layers: input, 1 hidden, and 1 output layers. \n",
    "<a name=\"NeuralNetwork-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check that:\n",
    "    \n",
    "* there are 3 layers\n",
    "* the input layers has 2 neurons\n",
    "* the hidden layer has 5 neurons\n",
    "* the output has 1 neuron\n",
    "<a name=\"Nlayers-1\"></a><a name=\"Npe-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers = 3\n",
      "Input layer dimension = 2\n",
      "Hidden layer dimension = 5\n",
      "Output layer dimension = 1\n"
     ]
    }
   ],
   "source": [
    "print(F\"Number of layers = {ANN.Nlayers}\")\n",
    "print(F\"Input layer dimension = {ANN.Npe[0]}\")\n",
    "print(F\"Hidden layer dimension = {ANN.Npe[1]}\")\n",
    "print(F\"Output layer dimension = {ANN.Npe[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation simply creates a collection of the weights and biases, that are stored in the matrices `W` and `B` correspondingly\n",
    "\n",
    "Also, this operation initializes the storage for the deltas of these parameters - `dW` and `dB`\n",
    "\n",
    "Note that matrices `W[0]` and `B[0]` are irrelevant (junk), are not used and are only needed for the consistency of the implementation with the common ways the ANN theory is described in the literature\n",
    "<a name=\"W-1\"></a><a name=\"B-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "W[2]\n",
      "0.0  0.0  0.0  0.0  0.0  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "B[2]\n",
      "0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"dWold-1\"></a><a name=\"dBold-1\"></a><a name=\"grad_w-1\"></a><a name=\"grad_b-1\"></a><a name=\"grad_w_old-1\"></a><a name=\"grad_b_old-1\"></a>\n",
    "In addition, the `NeuralNetwork` class objects hold other variables:\n",
    "\n",
    "* `dWold` and `dBold` - the deltas of weights and biases from the previous step\n",
    "* `grad_w` and `grad_b` - the gradients of the total error function (cost function) w.r.t. the corresponding weights and biases.\n",
    "* `grad_w_old` and `grad_b_old` - the gradients from the previous step\n",
    "\n",
    "Here, we output just one of these matrices. Of course, these matrices are still zero, since no actual gradient calculations of steps of the training has been completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "dB[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "dWold[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "dBold[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "grad_w[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "grad_b[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "grad_w_old[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "grad_b_old[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"dW[1]\"); data_outs.print_matrix(ANN.dW[1])\n",
    "print(\"dB[1]\"); data_outs.print_matrix(ANN.dB[1])\n",
    "\n",
    "print(\"dWold[1]\"); data_outs.print_matrix(ANN.dWold[1])\n",
    "print(\"dBold[1]\"); data_outs.print_matrix(ANN.dBold[1])\n",
    "\n",
    "print(\"grad_w[1]\"); data_outs.print_matrix(ANN.grad_w[1])\n",
    "print(\"grad_b[1]\"); data_outs.print_matrix(ANN.grad_b[1])\n",
    "\n",
    "print(\"grad_w_old[1]\"); data_outs.print_matrix(ANN.grad_w_old[1])\n",
    "print(\"grad_b_old[1]\"); data_outs.print_matrix(ANN.grad_b_old[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 OR Gate\n",
    "\n",
    "\n",
    "| Input A | Input B |  Output A and B |\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  1  |\n",
    "|  1  |  0  |  1  |\n",
    "|  1  |  1  |  0  |\n",
    "\n",
    "For the numerical convenience, the inputs and outputs are rescaled down to the [0.0, 0.5] range\n",
    "\n",
    "The AND truth table can be summarized as 4 inputs and 4 outputs. Each input and output constitute a column of the corresponding matrix. The length of each column (the number of rows in the matrices) corresponds to the dimensionalty of the input (2 - for the A and B) and output (A and B representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = MATRIX(2, 4)\n",
    "outputs = MATRIX(1, 4)\n",
    "\n",
    "# Pattern 0\n",
    "inputs.set(0, 0, 0.0)\n",
    "inputs.set(1, 0, 0.5)\n",
    "outputs.set(0, 0, 0.5)\n",
    "\n",
    "# Pattern 1\n",
    "inputs.set(0, 1, 0.5)\n",
    "inputs.set(1, 1, 0.0)\n",
    "outputs.set(0, 1, 0.5)\n",
    "\n",
    "# Pattern 2\n",
    "inputs.set(0, 2, 0.0)\n",
    "inputs.set(1, 2, 0.0)\n",
    "outputs.set(0, 2, 0.0)\n",
    "\n",
    "# Pattern 3\n",
    "inputs.set(0, 3, 0.5)\n",
    "inputs.set(1, 3, 0.5)\n",
    "outputs.set(0, 3, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to initialize the values of the weights and biases of the ANN\n",
    "<a name=\"Random-1\"></a><a name=\"normal-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "-0.07477622111046436  0.08395055949535368  \n",
      "-0.06434766339596208  -0.07365141637365749  \n",
      "0.09341565459689222  -0.01689948763387337  \n",
      "0.1547557965208588  0.08669922337979942  \n",
      "0.18312862418540063  0.17260067582517552  \n",
      "W[2]\n",
      "-0.04538204939678809  -0.10064546134452718  -0.13108375773470865  -0.11929342294018336  -0.2104164161703214  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.16595866588089658  \n",
      "-0.04176419336060002  \n",
      "0.13437233940011895  \n",
      "0.013877779733474208  \n",
      "0.20766988153254023  \n",
      "B[2]\n",
      "-0.07033053606495225  \n"
     ]
    }
   ],
   "source": [
    "rnd = Random()\n",
    "\n",
    "for L in range(1, ANN.Nlayers):\n",
    "    for i in range(ANN.Npe[L]):\n",
    "        for j in range(ANN.Npe[L-1]):\n",
    "            ANN.W[L].set(i, j, 0.1*rnd.normal())\n",
    "        ANN.B[L].set(i, 0, 0.1*rnd.normal() )\n",
    "        \n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can also be done with the help of the auxiliary function `init_weights_biases_normal` or `init_weights_biases_uniform`:\n",
    "\n",
    "    void init_weights_biases_uniform(Random& rnd, double left_w, double right_w, double left_b, double right_b);\n",
    "    void init_weights_biases_normal(Random& rnd, double scaling_w, double shift_w, double scaling_b, double shift_b);\n",
    " \n",
    "<a name=\"init_weights_biases_normal-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "0.07527230517978706  0.022031498744903302  \n",
      "-0.16425312472467501  0.08467514393299172  \n",
      "-0.010559248734041673  0.09015262815060981  \n",
      "-0.21000583895578304  0.0648939759991747  \n",
      "0.062099592148826224  0.06434395069103375  \n",
      "W[2]\n",
      "0.11461402592365191  0.09121337072615093  0.027191361927926923  0.12181633200119134  -0.09600636619256  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "-0.08124118109423346  \n",
      "0.23442900424232804  \n",
      "-0.1438027070441978  \n",
      "-0.07410769753918656  \n",
      "0.0017155856537309742  \n",
      "B[2]\n",
      "0.14328738176910125  \n"
     ]
    }
   ],
   "source": [
    "ANN.init_weights_biases_normal(rnd, 0.1, 0.0, 0.1, 0.0)\n",
    "\n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights and biases using random numbers sampled from a uniform distribution.\n",
    "<a name=\"init_weights_biases_uniform-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "-0.054751709268778426  0.0962921458279212  \n",
      "-0.09730533109852361  -0.05867930481148852  \n",
      "0.023265211341560454  -0.020179074499839494  \n",
      "-0.013230265543437689  0.08278720303568393  \n",
      "0.03371601250661352  -0.02874963988026122  \n",
      "W[2]\n",
      "0.045229321506446846  -0.026118213183301597  0.02180477880025504  0.05060311926091235  0.06994463874490217  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "-0.04750003197579647  \n",
      "-0.08949619605648154  \n",
      "-0.0826776044362586  \n",
      "0.09226557947335093  \n",
      "0.058773784879024044  \n",
      "B[2]\n",
      "-0.026668278745686766  \n"
     ]
    }
   ],
   "source": [
    "ANN.init_weights_biases_uniform(rnd, -0.1, 0.1, -0.1, 0.1)\n",
    "\n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving and loading ANNs\n",
    "<a name=\"save_load-1\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "<a name=\"save-1\"></a>\n",
    "The state of the ANN can be saved into a file of XML format using the `save` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN.save(\"my_ann.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"NeuralNetwork-2\"></a>\n",
    "\n",
    "A new ANN can be created from a file using the corresponding constructor. \n",
    "\n",
    "    NeuralNetwork(std::string xml_filename);\n",
    "  \n",
    "This version constructor reads in the xml file stored by the `save` function above and creates a new ANN instance with these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN1 = NeuralNetwork(\"my_ann.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the correctness of the ANN construction by comparing the output below with the one at the end of the section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "-0.054751709268778426  0.0962921458279212  \n",
      "-0.09730533109852361  -0.05867930481148852  \n",
      "0.023265211341560454  -0.020179074499839494  \n",
      "-0.013230265543437689  0.08278720303568393  \n",
      "0.03371601250661352  -0.02874963988026122  \n",
      "W[2]\n",
      "0.045229321506446846  -0.026118213183301597  0.02180477880025504  0.05060311926091235  0.06994463874490217  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "-0.04750003197579647  \n",
      "-0.08949619605648154  \n",
      "-0.0826776044362586  \n",
      "0.09226557947335093  \n",
      "0.058773784879024044  \n",
      "B[2]\n",
      "-0.026668278745686766  \n"
     ]
    }
   ],
   "source": [
    "print(\"W[0]\"); data_outs.print_matrix(ANN1.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN1.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN1.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN1.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN1.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN1.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load-1\"></a>\n",
    "We can also simply load the ANN state to a previously created ANN, which could be an empty one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "-0.054751709268778426  0.0962921458279212  \n",
      "-0.09730533109852361  -0.05867930481148852  \n",
      "0.023265211341560454  -0.020179074499839494  \n",
      "-0.013230265543437689  0.08278720303568393  \n",
      "0.03371601250661352  -0.02874963988026122  \n",
      "W[2]\n",
      "0.045229321506446846  -0.026118213183301597  0.02180477880025504  0.05060311926091235  0.06994463874490217  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "-0.04750003197579647  \n",
      "-0.08949619605648154  \n",
      "-0.0826776044362586  \n",
      "0.09226557947335093  \n",
      "0.058773784879024044  \n",
      "B[2]\n",
      "-0.026668278745686766  \n"
     ]
    }
   ],
   "source": [
    "ANN2 = NeuralNetwork()\n",
    "ANN2.load(\"my_ann.xml\")\n",
    "\n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN2.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN2.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN2.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN2.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN2.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN2.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "<a name=\"training\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "To train the ANN on a given set of patterns, we use two key functions: `propagate` and `back_propagate`, which take the signatures:\n",
    "\n",
    "    vector<MATRIX> propagate(MATRIX& input);\n",
    "    double back_propagate(vector<MATRIX>& Y, MATRIX& target);\n",
    "\n",
    "The `propagate` function takes a given input (which could be as many patterns as needed) and computes the outputs on each layer for each pattern. The results are returned as the lists of matrices.\n",
    "\n",
    "The `back_propagate` function takes the output in each layer (as returned by the `propagate` function) as well as the expected target output and computes the error on each layer (and the corresponding derivatives of the weights and biases), starting from the last (output) layer and working its way down to the first one. The error is thus propagated backwards, hence the name. \n",
    "\n",
    "As a result, the procedure updates the `dW` and `dB` values stored internally in the ANN object. The function also returns the error in the last layer to facilitae the tracking of the progress.\n",
    "\n",
    "Note that if there are many patterns are given, the `dW` and `dB` variables are computed as the average over those values over all the patterns.\n",
    "<a name=\"dW-1\"></a><a name=\"dB-1\"></a><a name=\"propagate-1\"></a><a name=\"back_propagate-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 0.031249234301378517\n",
      "dW[0]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "dW[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "dW[2]\n",
      "0.0  0.0  0.0  0.0  0.0  \n",
      "dB[0]\n",
      "0.0  \n",
      "0.0  \n",
      "dB[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "dB[2]\n",
      "0.0  \n"
     ]
    }
   ],
   "source": [
    "Y = ANN.propagate(inputs)\n",
    "res = ANN.back_propagate(Y, outputs)\n",
    "print(F\"Error = {res}\")\n",
    "\n",
    "print(\"dW[0]\"); data_outs.print_matrix(ANN.dW[0])\n",
    "print(\"dW[1]\"); data_outs.print_matrix(ANN.dW[1])\n",
    "print(\"dW[2]\"); data_outs.print_matrix(ANN.dW[2])\n",
    "\n",
    "print(\"dB[0]\"); data_outs.print_matrix(ANN.dB[0])\n",
    "print(\"dB[1]\"); data_outs.print_matrix(ANN.dB[1])\n",
    "print(\"dB[2]\"); data_outs.print_matrix(ANN.dB[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can formulate a simple procedure perform the simple gradient descent optimization of the weights and biases.\n",
    "\n",
    "Note that `grad_w` and `grad_b` are the positive gradients of the error w.r.t. to those parameters. So, in the gradiens descent algorithm, these come with the \"-\" sign\n",
    "\n",
    "Naturally, we don't want to plot all the stuff, only once in a while. \n",
    "<a name=\"W-2\"></a><a name=\"B-2\"></a><a name=\"grad_w-2\"></a><a name=\"grad_b-2\"></a><a name=\"propagate-2\"></a><a name=\"back_propagate-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0  error = 0.031252797524097986\n",
      "0.25108519370767135  0.24889753549256222  0.2459088811122869  0.2540475819794994  \n",
      "epoch = 1  error = 0.03125226210174958\n",
      "0.250932299096209  0.24905292178084287  0.24620596117601512  0.2537549325533576  \n",
      "epoch = 2  error = 0.03125182778243052\n",
      "0.2508030889668894  0.24918427888767763  0.2464688387185452  0.2534957738792456  \n",
      "epoch = 3  error = 0.031251470006172566\n",
      "0.25069318141450264  0.2492960704192284  0.2467032301789913  0.2532645379206682  \n",
      "epoch = 4  error = 0.03125117110497025\n",
      "0.2505991860175229  0.24939174626058497  0.24691356248173366  0.25305690791776425  \n",
      "epoch = 5  error = 0.031250918106224726\n",
      "0.25051843806547497  0.2494740152450607  0.24710331940483402  0.2528694833430638  \n",
      "epoch = 6  error = 0.03125070130842\n",
      "0.2504488141482428  0.24954503410802326  0.2472752821859606  0.252699546867904  \n",
      "epoch = 7  error = 0.03125051333443672\n",
      "0.25038860142501224  0.24960654128160126  0.24743170018848118  0.2525448988790063  \n",
      "epoch = 8  error = 0.031250348488338915\n",
      "0.2503364031622858  0.24965995346626474  0.24757441421608573  0.2524037377849541  \n",
      "epoch = 9  error = 0.031250202309637196\n",
      "0.2502910693029321  0.24970643654204236  0.24770494708959626  0.25227457201644826  \n",
      "epoch = 10  error = 0.03125007125882899\n",
      "0.25025164464221594  0.24974695844910308  0.24782457116000314  0.25215615437779726  \n",
      "epoch = 11  error = 0.031249952491899234\n",
      "0.2502173296029992  0.2497823291787342  0.24793435929566818  0.2520474324288274  \n",
      "epoch = 12  error = 0.03124984369615992\n",
      "0.25018745016786503  0.2498132314049957  0.24803522384571688  0.2519475105402737  \n",
      "epoch = 13  error = 0.03124974296905225\n",
      "0.2501614345612144  0.2498402442232682  0.24812794673296681  0.251855620568046  \n",
      "epoch = 14  error = 0.03124964872747287\n",
      "0.2501387949718403  0.24986386174576547  0.24821320291968432  0.251771098971405  \n",
      "epoch = 15  error = 0.031249559639069557\n",
      "0.2501191130844043  0.24988450781382066  0.24829157886489744  0.25169336880433185  \n",
      "epoch = 16  error = 0.031249474569536775\n",
      "0.25010202852084906  0.24990254774583773  0.24836358715660747  0.25162192543095074  \n",
      "epoch = 17  error = 0.031249392541687702\n",
      "0.25008722952762413  0.24991829779931127  0.24842967819433726  0.251556325114252  \n",
      "epoch = 18  error = 0.031249312703276848\n",
      "0.2500744454126057  0.24993203285340354  0.24849024957674795  0.2514961758414081  \n",
      "epoch = 19  error = 0.031249234301378517\n",
      "0.25006344035726624  0.24994399269415668  0.24854565368887016  0.2514411299044263  \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "steps_per_epoch = 5000\n",
    "dt = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    res, Y = 0.0, None\n",
    "    for i in range(steps_per_epoch):\n",
    "    \n",
    "        for L in range(ANN.Nlayers):\n",
    "            ANN.W[L] = ANN.W[L] - dt * ANN.grad_w[L]\n",
    "            ANN.B[L] = ANN.B[L] - dt * ANN.grad_b[L]\n",
    "\n",
    "        Y = ANN.propagate(inputs)\n",
    "        res = ANN.back_propagate(Y, outputs)\n",
    "        \n",
    "    print(F\"epoch = {epoch}  error = {res}\")\n",
    "    \n",
    "    data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After enough steps and cycles, we can compute the ANN recall (prediction) using the current state of the ANN parameters.\n",
    "\n",
    "As an example, we use the input that was also used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ANN.propagate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25006344035726624  0.24994399269415668  0.24854565368887016  0.2514411299044263  \n"
     ]
    }
   ],
   "source": [
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the results are pretty close to our expectations, although not ideal yet. More careful training, potentially with a more complex ANN architecture would be necessary to achieve that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have utilized all of our training examples in each step. This is called **batch** training\n",
    "\n",
    "However, sometimes it is adwantageous to use randomly selected subsets of the training examples in each step. This is called **online** training, and the number of examples presented at each time is called **epoch size**. In the above example of the batch training, we used an epoch size of 4.\n",
    "\n",
    "Let's consider smaller epoch sizes.\n",
    "\n",
    "In order to implement such a functionality, we need a procedure to select random sequences of numbers. This can be done with the help of `randperm` function, which takes the signature:\n",
    "\n",
    "    int randperm(int size,int of_size,vector<int>& result)\n",
    "    \n",
    "For instance, if we want to create a random sequence of 3 numbers from 5 numbers [0, 1, 2, 3, 4], we do:\n",
    "<a name=\"randperm-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "res = intList()\n",
    "randperm(3, 5, res)\n",
    "print( Cpp2Py(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to formulate the algorithm.\n",
    "\n",
    "<a name=\"pop_submatrix-1\"></a>\n",
    "Note how we use the `pop_submatrix` function to take certain columns (as defined by the `subset` variable) out of the full matrix of all inputs. We do such \"extraction\" for both inputs and outputs, congruently.\n",
    "<a name=\"randperm-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0  error = 0.029092859857135715\n",
      "0.2589021254774086  0.2586636075324908  \n",
      "epoch = 1  error = 0.029484715440837285\n",
      "0.24277001170654844  0.2429024149310819  \n",
      "epoch = 2  error = 0.03122327968435739\n",
      "0.2559560589553733  0.2556084380000714  \n",
      "epoch = 3  error = 0.031266429017724987\n",
      "0.26043465517221104  0.2601425794222132  \n",
      "epoch = 4  error = 0.03120365659655052\n",
      "0.24921002766619257  0.24958237183293663  \n",
      "epoch = 5  error = 0.03121894895276247\n",
      "0.24462636585282185  0.24498286931915486  \n",
      "epoch = 6  error = 0.031313716319489746\n",
      "0.2578110082093723  0.2580685132545198  \n",
      "epoch = 7  error = 0.03130732647878626\n",
      "0.23997205046217615  0.2400307717237521  \n",
      "epoch = 8  error = 0.03141078535806338\n",
      "0.26851106752245935  0.26843251586322797  \n",
      "epoch = 9  error = 0.02857528665537815\n",
      "0.2608067053975286  0.26106964521281956  \n",
      "epoch = 10  error = 0.028241318101048955\n",
      "0.23757135431298665  0.23775012936713447  \n",
      "epoch = 11  error = 0.031328454565097565\n",
      "0.25694629640074956  0.2573688314988394  \n",
      "epoch = 12  error = 0.031268029773500544\n",
      "0.23895815650544494  0.23928728735728777  \n",
      "epoch = 13  error = 0.03131740463243249\n",
      "0.2567825878830629  0.2571281956006963  \n",
      "epoch = 14  error = 0.03122313022843857\n",
      "0.2555877535142755  0.2559356208553157  \n",
      "epoch = 15  error = 0.03369393746720942\n",
      "0.259688624527347  0.2594948326266484  \n",
      "epoch = 16  error = 0.03130536425804642\n",
      "0.24915705769115112  0.24871884743404638  \n",
      "epoch = 17  error = 0.03119210483797258\n",
      "0.259074074425287  0.25989784817171846  \n",
      "epoch = 18  error = 0.031188004603238877\n",
      "0.24668890255906445  0.24722222453870693  \n",
      "epoch = 19  error = 0.03173035368515302\n",
      "0.2518289557564263  0.2519991900448095  \n",
      "0.25262258760790257  0.2512061407607205  0.2519991900448095  0.2518289557564263  \n"
     ]
    }
   ],
   "source": [
    "ANN2 = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )\n",
    "ANN2.init_weights_biases_uniform(rnd, -0.1, 0.1, -0.1, 0.1)\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "steps_per_epoch = 5000\n",
    "epoch_size = 2\n",
    "n_patterns = 4\n",
    "dt = 0.01\n",
    "\n",
    "input_subset = MATRIX(2, epoch_size)\n",
    "output_subset = MATRIX(1, epoch_size)\n",
    "subset = intList()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    res, Y = 0.0, None\n",
    "    for i in range(steps_per_epoch):    \n",
    "        for L in range(ANN.Nlayers):\n",
    "            ANN2.W[L] = ANN2.W[L] - dt * ANN2.grad_w[L]\n",
    "            ANN2.B[L] = ANN2.B[L] - dt * ANN2.grad_b[L]\n",
    "            \n",
    "        # Make a random selection of the training patterns\n",
    "        randperm(epoch_size, n_patterns, subset)\n",
    "        \n",
    "        # Extract the corresponding matrices from the inputs and outputs\n",
    "        pop_submatrix(inputs, input_subset, Py2Cpp_int( [0, 1] ), subset )\n",
    "        pop_submatrix(outputs, output_subset, Py2Cpp_int( [0] ), subset )\n",
    "\n",
    "        Y = ANN2.propagate(input_subset)\n",
    "        res = ANN2.back_propagate(Y, output_subset)\n",
    "        \n",
    "    print(F\"epoch = {epoch}  error = {res}\")\n",
    "    \n",
    "    data_outs.print_matrix(Y[2])\n",
    "    \n",
    "Y = ANN2.propagate(inputs)\n",
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this batch training are quite better than before.\n",
    "\n",
    "Apparently, the above simple procedure can be run as a sinlge function `train`\n",
    "<a name=\"train-1\"></a><a name=\"mlp-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999999999997983  0.4999999999998161  1.8673951274195133e-13  2.1538326677728037e-13  \n"
     ]
    }
   ],
   "source": [
    "ANN3 = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )\n",
    "ANN3.init_weights_biases_uniform(rnd, -1.1, 1.1, -1.1, 1.1)\n",
    "\n",
    "params = { \"num_epochs\":20, \n",
    "           \"steps_per_epoch\":5000, \n",
    "           \"epoch_size\":2, \"learning_rate\":0.01, \n",
    "           \"learning_method\":1, \n",
    "           \"verbosity\":1 }\n",
    "\n",
    "err = ANN3.train(rnd, params, inputs, outputs )\n",
    "\n",
    "Y = ANN3.propagate(inputs)\n",
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference, here we summarize the keywords that the `train` function parameters dictionary can have:\n",
    "\n",
    "* `learning_method` - selects the method for the ANN training, the selection goes as following\n",
    "\n",
    "  -  1 [default] Back Propagation (BProp) and options, no momentum, Algorithm 1 of [2], neither purple nor green\n",
    "  - 11 BProp with L2 regularization - Algorithm 1 of [2], purple option\n",
    "  - 12 BProp with decoulpled decay  - Algorithm 1 of [2], green option\n",
    "  - 13 [under development] Adam with L2 regularization - Algorithm 2 of [2], purple option\n",
    "  - 14 [under development] Adam with decoupled decay - Algorithm 2 of [2], green option\n",
    "  - 2  Resilient Propagation without weight-backtracking (RProp-) - Eq. 1 of [3] + section 2.2\n",
    "  - 21 [under development] Resilient Propagation with weight-backtracking (RProp+) - Eq. 1 of [3] + section 2.1\n",
    "  - 22 [under development] Modified RProp- (iRprop-)\n",
    "  - 23 [under development] Modified RProp+ (iRprop+)\n",
    "\n",
    "* `learning_rate` [default: 0.001] - the timestep size,  $\\alpha$ in algorithm (1) of [2]\n",
    "  \n",
    "\n",
    "* `momentum_term` [default: 0.0] - the momentum term control parameter, $\\beta_1$ in algorithm (1) of [2]\n",
    "  \n",
    "  \n",
    "* `weight_decay_lambda` [default: 0.0] - the L2 regularization factor  L_new = L_old + lambda * w^T w, $\\lambda$ in algorithm (1) of [2]\n",
    "\n",
    "\n",
    "* `etha` [default: 1.0] - the control parameter for the decoupled weight decay, the $\\eta_t$ in algorithm (1) of [2]\n",
    "\n",
    "\n",
    "* `num_epochs` [default: 1] - the number of training epochs to do\n",
    "\n",
    "\n",
    "* `steps_per_epoch` [default: 1] - the number of steps per epoch \n",
    "\n",
    "\n",
    "* `epoch_size` [default: 1] - how many patterns to include in each step of each epoch\n",
    "\n",
    "\n",
    "* `verbosity`  [default : 0] - the level of additional printing\n",
    "\n",
    "\n",
    "* `error_collect_frequency` [default: 1] - how often to collect the information on errors, 1 means every training step\n",
    "\n",
    "\n",
    "* `a_plus` [default: 1.1] - the RProp step size multiplication (increase) parameter - good for up to 1.2, corresponds to $\\eta^{+}$ parameter of [3]\n",
    "\n",
    "\n",
    "* `a_minus` [default: 0.6] - the RProp step size multiplication (decrease) parameter - good for down to 0.5, corresponds to $\\eta^{-}$ parameter of [3]\n",
    "\n",
    "\n",
    "* `dB_min` [default: $0.1\\times $`learning_rate` ] - the minimal acceptable timestep for B updates in the RProp family of methods\n",
    "\n",
    "\n",
    "* `dB_max` [default: $1.0\\times $`learning_rate` ] - the maximal acceptable timestep for B updates in the RProp family of methods\n",
    "\n",
    "\n",
    "* `dW_min` [default: $0.1\\times $`learning_rate` ] - the minimal acceptable timestep for W updates in the RProp family of methods\n",
    "\n",
    "\n",
    "* `dW_max` [default: $1.0\\times $`learning_rate` ] - the maximal acceptable timestep for W updates in the RProp family of methods\n",
    "\n",
    "\n",
    "\n",
    "  **References for the above parameters**:\n",
    "  1. [http://page.mi.fu-berlin.de/rojas/neural/chapter/K8.pdf](http://page.mi.fu-berlin.de/rojas/neural/chapter/K8.pdf)\n",
    "  2. [https://arxiv.org/pdf/1711.05101.pdf](https://arxiv.org/pdf/1711.05101.pdf)\n",
    "  3. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1332](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1332)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error tracking\n",
    "<a name=\"errors\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "The `train` function returns the error object that can be used to track the progress or assess the quality of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "liblibra_core.doubleList"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6c91df64a8>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiGElEQVR4nO3dfZRV9X3v8fdnHpgZHmQGnFEYRECNIy6jaWiqNTVxTKpJDbmrqeY2MbW6gql32ZqVNr0hWmvB3NiaqHc1K7dqjO2VdMWrTauuxmgD1KQKNmg0ZVQUAhERkednhnn43j/2OXA8c5izYR7OA5/XWrMO7P37nfM9Rzyf2fv3++2tiMDMzCxXTakLMDOz8uNwMDOzARwOZmY2gMPBzMwGcDiYmdkADgczMxugrtQFjLYTTzwxZsyYUeoyzMxK7vnnn98SEa2F9h134TBjxgxWrFhR6jLMzEpO0q+OtM+nlczMbACHg5mZDeBwMDOzARwOZmY2gMPBzMwGcDiYmdkADocUevr6eWPrPvZ295a6FDOzUeFwSOG/NuzkojuW8tzaraUuxcxsVDgcUmhvbgJgw44DJa7EzGx0OBxSaB3fwJjaGjZs31/qUszMRoXDIYWaGjGluZENOxwOZnZ8cDikNHViExu27yt1GWZmo8LhkFJ7SxNveczBzI4TDoeU2pub2LT7AAd7+0tdipnZiHM4pNTe0kQEvL3TRw9mVv0cDikdns7qQWkzq34Oh5QcDmZ2PHE4pDSluRHAax3M7LiQKhwknSLpEUk7Je2S9ANJ01P2bZR0h6SNkvZLWibpogLtviTp8Uy7kHTrIM85T9KrkrolrZL0R2lqGYqGulpaJzTwlo8czOw4UDQcJI0FlgAdwNXA54AzgKWSxqV4jfuBecAtwOXARuBJSefltZsHtAH/UqSeecA9wD8BlwEPA9+WdH2KWoakvbnJp5XM7LhQl6LNPGAWcGZErAaQ9AvgdeALwJ1H6ijpXOAzwLUR8UBm29NAF7AAmJvT/OyI6JdUBxQ8Esjs+xrwYETclNm8VNJUYKGk70RET4r3dEzaW5p4+a1dI/X0ZmZlI81ppbnA8mwwAETEWuAZ4JMp+vYAD+X07QW+D1wqqSFne5oFBBcArcCivO0PApOBD6Z4jmOWPXLo74+RfBkzs5JLEw5nAysLbO8CZqfouzYi8q870QWMAU5P8fr5z0eBeroyj8XqGZL25iYO9vazde/BkXwZM7OSSxMOk4DtBbZvA1qG0De7/2hk2+c/56DPJ+k6SSskrdi8efNRvuRhns5qZseLtFNZC51HUYp+GkLfIz3fkeo5ooi4NyLmRMSc1tbWY3zpZMwBPJ3VzKpfmnDYTuHfyFsofFSQa9sgfbP7j8aRjhAm5e0fEVMzRw6ezmpm1S5NOHRx+Fx/rtnAyyn6zsxMh83vexBYPbBL0eejQD3ZsYZi9QzJxKZ6JjTU+bSSmVW9NOHwGHC+pFnZDZJmABdm9hXrWw9ckdO3Dvg08FREdB9lvcuALcBn87ZfRXLU8MxRPt9Ra29p4k2fVjKzKpdmncN9wA3Ao5JuJjnfvxBYT7IYDQBJpwJrgAURsQAgIl6U9BBwt6R6YC1wPTCTvC94SXOAGRwOrNmSfi/z5x9GxL6I6JH0FySL3jYAPwY6gWuBP46IEZ9GNLW5yaeVzKzqFQ2HiNgrqRO4i2Q9gYDFwBcjYk9OUwG1DDwauYZk4dptQDPwEnBZRLyQ1+4GkhXYWVdw+IhjJrAuU8/fSQrgT4EvA28AN0TEt4u9l+HQ3tzE878qNtRiZlbZ0hw5EBFvAJ8q0mYdBWYhRcR+4EuZn8H6/yHwhynruYeco5bR1N7SxM79Pezp7mV8Q6qPz8ys4viqrEcpO2PJ01nNrJo5HI5Su6ezmtlxwOFwlKZlFsK96XAwsyrmcDhKreMbqK+VTyuZWVVzOBylmhoxZaKns5pZdXM4HAPf9MfMqp3D4Ri0tzT5tJKZVTWHwzGY2tzEpt0H6OlLc38iM7PK43A4BtOam4iAt3ceKHUpZmYjwuFwDLL3dfAF+MysWjkcjsFU3xHOzKqcw+EYTJnYCHiVtJlVL4fDMWisr6V1QoNnLJlZ1XI4HCOvdTCzauZwOEbtvumPmVUxh8Mxam9JjhwiotSlmJkNO4fDMWpvbqK7t58te0b8zqRmZqPO4XCMpvq+DmZWxRwOx6jdax3MrIo5HI5RdpW0p7OaWTVyOByjExrrGN9Q5yMHM6tKDodjJMlrHcysajkchsD3dTCzapUqHCSdIukRSTsl7ZL0A0nTU/ZtlHSHpI2S9ktaJumiAu1qJM2XtE7SAUkvSfpUgXZjJf2VpNcyz7de0v+VNCNNPcPJRw5mVq2KhoOkscASoAO4GvgccAawVNK4FK9xPzAPuAW4HNgIPCnpvLx2C4FbgW8BHwOWAw9L+nheu+8AXwbuAz4O3AxcBCyWND5FPcNmanMTO/f3sKe7dzRf1sxsxNWlaDMPmAWcGRGrAST9Angd+AJw55E6SjoX+AxwbUQ8kNn2NNAFLADmZra1AX8G3B4R38h0XyrpdOB24IeZdk3AlcDfRMQdOa+zCXgCuBB4MtU7HwbZGUtv7djPe06aMFova2Y24tKcVpoLLM8GA0BErAWeAT6Zom8P8FBO317g+8Clkhoymy8FxgCL8vovAs6RNDPz9zqgFtiV127HUbyfYXNorYPHHcysyqT5Mj0bWFlgexcwO0XftRGxr0DfMcDpOe26gdUF2pF9nYjYDTwI/ImkiyWNl3Q2cAfwErC4+NsZPl4IZ2bVKk04TAK2F9i+DWgZQt/s/uzjjhh4Fbv8dgDXAP9MMg6ymyS46oGPRkTBCx1Juk7SCkkrNm/eXKTk9NomNFBfK4eDmVWdtKdhCl16VCn6KWXftO0AbgOuIhmj+BDJAPlk4IkjDZBHxL0RMSci5rS2tqYoO52aGjFloqezmln1STMgvZ13/+ae1ULho4Jc24BCU15bcvZnH1skKe/o4V3tMqeQvgJ8PiLuzzaS9BzwGvB54H8XqWlYTW1u9MX3zKzqpDly6CIZE8g3G3g5Rd+Zmemw+X0PcniMoQtoAE4r0I6c1zkn8/iz3EYR8TrJoPRZReoZdu3NY31aycyqTppweAw4X9Ks7IbMgrMLM/uK9a0HrsjpWwd8GngqIrozm39EEhafzet/FbAyMzsK4O3M4wdyG0l6D9AMbEjxfoZVe0sTm3YdoKevf7Rf2sxsxKQ5rXQfcAPwqKSbScYGFgLrgXuyjSSdCqwBFkTEAoCIeFHSQ8DdkuqBtcD1wExygiAi3pF0FzBf0m7gBZIA6eTd02V/SjIr6ZuSWoAVJKetbgZ2Av9w1J/AELU3N9If8PbOA5wyKf8AycysMhUNh4jYK6kTuItkGqlIpox+MSL25DQVyRqE/KORa4CvkQwkN5N8uV8WES/ktbsJ2APcCJwMrAKujIjHc2rpk3QJ8FXgOpKFdFuAZ4FbIuKNFO95WLU3J4GwYcd+h4OZVY00Rw5kvnQHXOcor806Cswuioj9wJcyP4P17yMJkNuKtNsK/Gnmp+R8Xwczq0a+KusQTZnYCHghnJlVF4fDEDXW13Li+AZPZzWzquJwGAbtLb50t5lVF4fDMJjW7FXSZlZdHA7DYGpzIxt27GfgpaHMzCqTw2EYtDc30d3bz9a9Ba/7Z2ZWcRwOw6C9JbPWwaeWzKxKOByGwdRmT2c1s+ricBgG0zKrpD2d1cyqhcNhGJzQVMf4hjre9GklM6sSDodhIIn2Zq91MLPq4XAYJr7pj5lVE4fDMPEqaTOrJg6HYdLePJYd+3rY291b6lLMzIbM4TBMstNZfWrJzKqBw2GYTMvc1+FNh4OZVQGHwzA5dEc4T2c1syrgcBgmrRMaqKuRB6XNrCo4HIZJbY2Y4umsZlYlHA7DqN33dTCzKuFwGEbtzWN9WsnMqoLDYRi1NzeyadcBevr6S12KmdmQOByGUXtLE/0Bb+88UOpSzMyGJFU4SDpF0iOSdkraJekHkqan7Nso6Q5JGyXtl7RM0kUF2tVImi9pnaQDkl6S9KkjPGeLpLslvSGpW9Kbkv4+TT0j6dB0Vp9aMrMKVzQcJI0FlgAdwNXA54AzgKWSxqV4jfuBecAtwOXARuBJSefltVsI3Ap8C/gYsBx4WNLH8+ppAf4D+AhwM/BR4M+A3SlqGVFeJW1m1aIuRZt5wCzgzIhYDSDpF8DrwBeAO4/UUdK5wGeAayPigcy2p4EuYAEwN7OtjeQL/vaI+Eam+1JJpwO3Az/MedqvA+OBcyJiV87276d4LyNqanOyStozlsys0qU5rTQXWJ4NBoCIWAs8A3wyRd8e4KGcvr0kX+SXSmrIbL4UGAMsyuu/CDhH0kyAzJHKHwDfyQuGstBYX8uJ4xt8WsnMKl6acDgbWFlgexcwO0XftRGxr0DfMcDpOe26gdUF2pHzOu8HmoBNmTGQ/ZL2SPqXbICUWntzo8PBzCpemnCYBGwvsH0b0DKEvtn92ccdERFF2k3NPH4D6CM5MrkOeB/w75ImFCpC0nWSVkhasXnz5iIlD43v62Bm1SDtVNb8L20ApeinlH3TtsvWuxb47xHxbxHxj8CVwHTgqkJFRMS9ETEnIua0tramKPvYtTc38daO/QzMOTOzypEmHLZz+Df3XC0UPirItW2Qvtn92ccWSflhkN9ua+bxx7lHGRHxHLCL5AiipNqbmzjQ08/WvQdLXYqZ2TFLEw5dJGMC+WYDL6foOzMzHTa/70EOjzF0AQ3AaQXakfM62TGII/1aXvKlydkZS57OamaVLE04PAacL2lWdoOkGcCFmX3F+tYDV+T0rQM+DTwVEd2ZzT8iCYvP5vW/CliZmR1FRLwJrAB+O/coQ9IFwAnAz1K8nxHV3uLprGZW+dKsc7gPuAF4VNLNJL+1LwTWA/dkG0k6FVgDLIiIBQAR8aKkh4C7JdWTjBVcD8wkJwgi4h1JdwHzJe0GXiAJkE4GTpf9CvAk8Iik7wCtwNeAV4F/PLq3P/ymeZW0mVWBouEQEXsldQJ3AQ+SDBIvBr4YEXtymgqoZeDRyDUkX963Ac3AS8BlEfFCXrubgD3AjcDJwCrgyoh4PK+exZI+QbKI7p+BvcC/Al+OiJJ/I5/QVMe4MbUOBzOraGmOHIiIN4CC1znKabOOAjOYMl/YX8r8DNa/jyRAbktRzxPAE8XalYKkZDqrTyuZWQXzVVlHQHuz1zqYWWVzOIyAqZm1DmZmlcrhMALaW5rYvq+HfQd7S12KmdkxcTiMgHZfndXMKpzDYQRMy6x1eNOnlsysQjkcRoBXSZtZpXM4jIC2CY3U1cinlcysYjkcRkBtjZji+zqYWQVzOIyQqRM9ndXMKpfDYYR4lbSZVTKHwwiZ1tzE27sO0NNX8quIm5kdNYfDCJna3ER/wKZdB0pdipnZUXM4jBDf18HMKpnDYYQcWiXtQWkzq0AOhxEy1ZfQMLMK5nAYIY31tZw4fgxv7XQ4mFnlcTiMoPbmJt70kYOZVSCHwwhqb/FNf8ysMjkcRlB2lXRElLoUM7Oj4nAYQe0tTRzo6Wfb3oOlLsXM7Kg4HEaQp7OaWaVyOIwg39fBzCqVw2EEHbojnGcsmVmFSRUOkk6R9IiknZJ2SfqBpOkp+zZKukPSRkn7JS2TdFGBdjWS5ktaJ+mApJckfarIc/+mpH5JIakuTT2jaWJTPePG1Pq0kplVnKLhIGkssAToAK4GPgecASyVNC7Fa9wPzANuAS4HNgJPSjovr91C4FbgW8DHgOXAw5I+foS66oF7gE0paigJSUxt9n0dzKzypPltex4wCzgzIlYDSPoF8DrwBeDOI3WUdC7wGeDaiHggs+1poAtYAMzNbGsD/gy4PSK+kem+VNLpwO3ADws8/ZcBAd8FvprifZSE1zqYWSVKc1ppLrA8GwwAEbEWeAb4ZIq+PcBDOX17ge8Dl0pqyGy+FBgDLMrrvwg4R9LM3I2STgNuAv5H5vnLVnuzb/pjZpUnTTicDawssL0LmJ2i79qI2Feg7xjg9Jx23cDqAu0o8Dr/B3gkIn5S5PVLrr2lie37eth3sLfUpZiZpZbmtNIkYHuB7duAliH0ze7PPu6IgUuJ89sh6SpgDskYSNlrz5nOenrbhBJXY2aWTtqprIWu/6AU/ZSyb6p2kiYB3wS+GhHvpHj9bL/rJK2QtGLz5s1puw2LbDh4OquZVZI04bCdnN/cc7RQ+Kgg17ZB+mb3Zx9bJOWHRn6720hmJ/0/Sc2SmoHGzL6JR5o9FRH3RsSciJjT2tpapOThdeiOcB6UNrMKkiYcukjGBPLNBl5O0XdmZjpsft+DHB5j6AIagNMKtCPndWYD5wBbSYJpO/A/M/u2AN8rUs+oa5vQSF2NPJ3VzCpKmnB4DDhf0qzsBkkzgAsz+4r1rQeuyOlbB3waeCoiujObf0QSFp/N638VsDIzOwrgi8DFeT//kNn3EeDmFO9nVNXWiJMnNnrGkplVlDQD0vcBNwCPSrqZZGxgIbCeZBEaAJJOBdYACyJiAUBEvCjpIeDuzKK1tcD1wExygiAi3pF0FzBf0m7gBZIA6SRnumxEvJhfnKQPZ/74dGaabNlpb/ZaBzOrLEXDISL2SuoE7gIeJBkkXgx8MSL25DQVUMvAo5FrgK+RjBc0Ay8Bl0XEC3ntbgL2ADcCJwOrgCsj4vGjfE9lp725iefWbive0MysTKS6HlFEvAEMep2jiFhHgRlMEbEf+FLmZ7D+fSQBcluamnL63Upy2Y2y1d7SxNsvHaC3r5+6Wl/r0MzKn7+pRkF7cxN9/cHbuw6UuhQzs1QcDqPg0HRWD0qbWYVwOIyCQzf92elwMLPK4HAYBYduF+ojBzOrEA6HUdBYX8uJ48d4OquZVQyHwyg5o20CP39jR6nLMDNLxeEwSjo72nj17d0+ejCziuBwGCUXd7QBsOTV1BeTNTMrGYfDKDmtdRynTh7LklfK9pbXZmaHOBxGiSQ6O9p4ds1W9h/sK3U5ZmaDcjiMos6ONrp7+3l2zZZSl2JmNiiHwyj6wMxJjBtTy2KPO5hZmXM4jKKGulp+64xWlr76DgNvl21mVj4cDqOss6ONjTsP8MrG3aUuxczsiBwOo+zDHck9rJe86llLZla+HA6jrG1CI+dOm+j1DmZW1hwOJXBxRxs/X7+DrXu6izc2MysBh0MJXNJxEhHw76s2l7oUM7OCHA4lcPbUE2id0MCSVT61ZGblyeFQAjU1ovPMNn6yajM9ff2lLsfMbACHQ4l0ntXG7u5efrZuW6lLMTMbwOFQIh88/UTG1Naw1LOWzKwMORxKZFxDHb8xa5IvpWFmZSlVOEg6RdIjknZK2iXpB5Kmp+zbKOkOSRsl7Ze0TNJFBdrVSJovaZ2kA5JekvSpvDZTJH1d0opMLZslLS70fJXgko42frl5L+u27C11KWZm71I0HCSNBZYAHcDVwOeAM4ClksaleI37gXnALcDlwEbgSUnn5bVbCNwKfAv4GLAceFjSx3PavB/4NPAo8HvAHwIHgH+XdHmKWspKZ8dJgG8AZGblR8UuACfpRuBO4MyIWJ3ZNhN4HfjziLhzkL7nAi8C10bEA5ltdUAXsCoi5ma2tQHrgdsj4i9z+i8GWiPivZm/NwN7IqI3p032+TZFRNEjiDlz5sSKFSuKNRs1H7nzaU4+oZFFn/+NUpdiZscZSc9HxJxC+9KcVpoLLM8GA0BErAWeAT6Zom8P8FBO317g+8Clkhoymy8FxgCL8vovAs7JhBERsSM3GHKe70WgPcV7KTuXdLTx3Nqt7OnuLd7YzGyUpAmHs4GVBbZ3AbNT9F0bEfsK9B0DnJ7TrhtYXaAdg72OpDHABcArRWopSxd3tNHTF/zH614tbWblI004TAK2F9i+DWgZQt/s/uzjjhh4jiu/XSG3AtOAvy5SS1l6/6ktnNBYx+JXPO5gZuUj7VTWQgMTStFPKfumbffundJngK8ACyPip4O0uy4zw2nF5s3l9Rt6fW0NHzqzjaWr3qG/3zcAMrPykCYctlP4N/cWCh8V5No2SN/s/uxji6T8MMhvd4ikTwB/D9yfO4hdSETcGxFzImJOa2trkZJHX2dHK1v2HOS/NuwsdSlmZkC6cOgiGRPINxt4OUXfmZnpsPl9D3J4jKELaABOK9CO/NeRdAnwMPDPwBeK1FD2PvSeNmqEF8SZWdlIEw6PAedLmpXdIGkGcGFmX7G+9cAVOX3rSNYqPBUR2Rsa/IgkLD6b1/8qYGVmdlS2/wUk6xwWA1dFRMVfuW7SuDH82vQW3x3OzMpGXYo29wE3AI9KuplkbGAhybqEe7KNJJ0KrAEWRMQCgIh4UdJDwN2S6oG1wPXATHKCICLekXQXMF/SbuAFkgDpJGe6rKQO4F+BLcAdwPtzz0RFxPKj/QDKxcUdbdzx5Co27TrASSc0lrocMzvOFT1yiIi9JF/SrwEPAt8j+ZLvjIg9OU0F1BZ4zmuAB4DbSL7YTwEui4gX8trdlGlzI/AkyZHJlRHxeE6b80nGIU4FlgLL8n4q1iVntQH4QnxmVhaKrpCuNuW2QjorIvjgXy9l9tQTuO8PCi5YNDMbVkNdIW2jQBIXd7TyzOotHOjpK3U5ZnaccziUkUs6TmLfwT6eW+sbAJlZaTkcysgFp02msb6GJa941pKZlZbDoYw01tdy4WknsmTVOxxvY0FmVl4cDmWm86w21m/bz+p39hRvbGY2QhwOZaazI5nS6tXSZlZKDocyM2ViE2dNOcF3hzOzknI4lKFLOtp4/lfb2bHvYKlLMbPjlMOhDF3c0UZff/D0a+V1eXEzO344HMrQeac0M2ncGJ9aMrOScTiUodoa8eEzW3n6tc309lX8RWfNrAI5HMpUZ0cbO/b18PP1O0pdipkdhxwOZeq3zmilrka+t7SZlYTDoUxNbKrn12dM8iW8zawkHA5lrLOjjVWbdrN+275Sl2JmxxmHQxnrzN4AaJWPHsxsdDkcytisE8cxY/JYT2k1s1HncChjyQ2A2nh2zVb2HewtdTlmdhxxOJS5SzpO4mBvP8+s3lrqUszsOOJwKHMfmDmJcWNqfWrJzEaVw6HMjamr4bfOaGXJq5t8AyAzGzUOhwrQeVYbm3Z10/XWrlKXYmbHCYdDBbj4zMyUVp9aMrNRkiocJJ0i6RFJOyXtkvQDSdNT9m2UdIekjZL2S1om6aIC7WokzZe0TtIBSS9J+tQRnnOepFcldUtaJemP0tRSqVonNHDutIk89fImz1oys1FRNBwkjQWWAB3A1cDngDOApZLGpXiN+4F5wC3A5cBG4ElJ5+W1WwjcCnwL+BiwHHhY0sfz6pkH3AP8E3AZ8DDwbUnXp6ilYl3+3qn814adnPtXT3Hl3y3jrn97jed+uZXu3r5Sl2ZmVUjFBjkl3QjcCZwZEasz22YCrwN/HhF3DtL3XOBF4NqIeCCzrQ7oAlZFxNzMtjZgPXB7RPxlTv/FQGtEvDen71vAExFxdU677wJzgSkR0TPY+5kzZ06sWLFi0Pdcjvr7g5+u3sKza7awbM1WVm7YSX9AY30Nc06dxAWnTeY3T5vMOe0Tqav12UIzK07S8xExp9C+uhT95wLLs8EAEBFrJT0DfJIkOAbr2wM8lNO3V9L3ga9IaoiIbuBSYAywKK//IuC7kmZGxFrgAqC1QLsHgWuADwJLU7ynilNTIz70nlY+9J5WAHbu7+E/1247FBZ3PLkKgPENdfzGzCQsLjhtMmedfAI1NSpl6WZWgdKEw9nAowW2dwFXpOi7NiLyrxzXRRIGp2f+fDbQDawu0A5gNrA20w5g5SDtqjIc8k1squejs0/io7NPAmDrnm6W//JwWCzODF43j63ngllJUJw/azKTx42hrraGuhpRVyvqa2ocHmY2QJpwmARsL7B9G9AyhL7Z/dnHHTHwHFehdhR4zvx2x53J4xv4nfdO4XfeOwWAjTv3s2zNVpat2cqza7byxMq3j9i3RlBXW0N9jd4VHHU1NdTXHt5WX1tDjQC9O0zyoyVvd4H9DiOz4TK1uYm//f33DfvzpgkHgEIDE2n+D1fKvkfT7kj1HLkI6TrgOoDp01NNsqp4UyY28bu/No3f/bVpRATrt+1nxa+2sae7l56+oLevn97+oKevn96+oKe/n76+GLCtty/o7e8/1Kc/75PP/w9RbAzL6/jMhldD3ciMMaYJh+0U/o28hcJHBbm2AYW+jVty9mcfWyQp7+ihUDsy9WzMaTcpb/+7RMS9wL2QDEgXqbnqSGL65LFMnzy21KWYWYVIEznZMYF8s4GXU/SdmZkOm9/3IIfHGLqABuC0Au3IeZ3s2EJ+PfntzMxsCNKEw2PA+ZJmZTdImgFcmNlXrG89OQPXmemonwaeysxUAvgRSVh8Nq//VcDKzEwlgGXAliO02wY8k+L9mJlZEWlOK90H3AA8KulmktPMC0nWJdyTbSTpVGANsCAiFgBExIuSHgLullRPMuPoemAmOV/wEfGOpLuA+ZJ2Ay+QBEgnyXTZbLseSX9BsuhtA/DjTJtrgT+OiIPH9jGYmVmuouEQEXsldQJ3kawnELAY+GJE7MlpKqCWgUcj1wBfA24DmoGXgMsi4oW8djcBe4AbgZOBVcCVEfF4Xj1/JymAPwW+DLwB3BAR3y76bs3MLJWiK6SrTaWukDYzG26DrZD2dRbMzGwAh4OZmQ3gcDAzswGOuzEHSZuBX5W6jgp2Isl0Yhsaf47Dw5/j0JwaEa2Fdhx34WBDI2nFkQawLD1/jsPDn+PI8WklMzMbwOFgZmYDOBzsaN1b6gKqhD/H4eHPcYR4zMHMzAbwkYOZmQ3gcLBBSfqwpCjws6PUtZUrSdMk/a2kZZL2ZT6vGQXatUj6jqQtkvZK+rGkc0pQcllK8zlKmnGEf58hqbk0lVeHtHeCM/sT4Gc5f+8tVSEV4HTgSuB54KfAb+c3UHKv1MdIrlD8xyQ3zpoPLJV0XkS8OXrllq2in2OOrzPwFgK7R6iu44LDwdJ6JSKWl7qICvGTiDgJQNLnKfylNhf4INAZEUszbZeRXNb+z0nC+HiX5nPM+qX/fQ4vn1YyG2YR0Z+i2VzgrWwwZPrtBB4n5x4mx7OUn6ONEIeDpfU9SX2Stkr6R0mF7g1u6Z0NrCywvQuYLmn8KNdT6b4uqVfSTkmPeexm6HxayYrZCXwTeBrYBbwP+CqwTNL7IuKdUhZXwSYB6wps35Z5bCG5+ZUNrpvkjpRPAZuBDpJ/n89K+kBEvFLK4iqZw8EGFRE/B36es+lpST8B/pPkvPjNJSms8onklruFtltKEbER+KOcTT+V9COSI7CbSO4vb8fAp5XsqGVu8foa8OulrqWCbSM5esjXknncPoq1VJWIWA/8B/73OSQOBztWR/rN19LpIhl3yDcbeCPv/ux29Pzvc4gcDnbUJM0B3gM8V+paKthjQLukD2U3SDoB+AQD5+vbUchMlrgQ//scEo852KAkfY9k7v0LwA6SAen5wAbgb0tXWXmT9HuZP74/8/ixzI2mNkfE0yQBsAxYJOnLHF4EJ+BvRrveclXsc5T0TZJfcpeRDEifSfI59gP/a7TrrSa+8J4NStJ84PeBU4GxwNvAE8BfZgYDrQBJR/of6+mI+HCmzSTgG8B/AxpJvuC+FBEvjUaNlaDY5yjpWuB6ktXUE0juCrcE+KuIWDVKZVYlh4OZmQ3gMQczMxvA4WBmZgM4HMzMbACHg5mZDeBwMDOzARwOZmY2gMPBzMwGcDiYmdkADgczMxvg/wOoyYU+2ypkRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sz = len(err)\n",
    "x = list( range(1,sz))\n",
    "y = Cpp2Py(err)\n",
    "\n",
    "plt.plot(x, y[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"error-1\"></a>\n",
    "\n",
    "In addition, one can use function `error` to comput the error for a given output, provided we know the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN3.error(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Train an ANN to learn the exclusinve OR (XOR) gate:\n",
    "\n",
    "| Input A | Input B |  Output A and B |\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  1  |\n",
    "|  0  |  1  |  1  |\n",
    "|  1  |  1  |  0  |\n",
    "\n",
    "Experiment with the ANN architecture and training parameters. Can you make the ANN with no hidden layers to learn this pattern?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Train an ANN to learn the quadratic function $y(x) = x^2$ on the [0, 5] interval. \n",
    "\n",
    "Hint: keep in mind that the output of the $tanh(x)$ function can be in the [-1, 1] interval, so you need to transform target y values into that interval. Even better, to something like [-0.5, 0.5]\n",
    "\n",
    "Also, the best learning happens where the slope of the transfer function isn't too close to zero, so it is a good idea to convert the input variables into another range, e.g. [-1, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
